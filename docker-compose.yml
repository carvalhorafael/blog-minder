services:
  crewai:
    container_name: crewai
    build: .
    tty: true
    stdin_open: true
    env_file:
      - .env
    environment:
      - SERPER_API_KEY=${SERPER_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - .:/app

  # ollama: #to use with Ollama by docker
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - 11434:11434
  #   volumes:
  #     - ollama:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: ${OLLAMA_GPU_DRIVER-nvidia}
  #             count: ${OLLAMA_GPU_COUNT-1}
  #             capabilities:
  #               - gpu

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ollama-webui:/app/backend/data
    # depends_on: #to use with Ollama by docker
    #   - ollama
    ports:
      - 8080:8080
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://host.docker.internal:11434 #to use with Ollama localhost
      - ENV=dev
      - WEBUI_AUTH=False
      - WEBUI_NAME=valiantlynx AI
      - WEBUI_URL=http://localhost:8080
      - WEBUI_SECRET_KEY=t0p-s3cr3t
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  # ollama: {} #to use with Ollama by docker
  ollama-webui: {}
